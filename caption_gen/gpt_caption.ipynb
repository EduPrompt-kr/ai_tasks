{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ec7dc-503d-43e1-87c7-f2281a75bbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Downloading openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (319 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.9.0 openai-1.70.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install aiofiles aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f305c-e8d5-4d7b-bbd5-6f06b1dde17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "\n",
      "main_objects: boy, soccer ball; main_object_attributes: red jersey, blue shorts, black shoes, medium size ball; location: soccer field; action: kicking; surroundings: grass, goalposts; background: bright sun, blue sky with clouds, trees\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# STEP 1: Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"nn\")  # Replace with your key\n",
    "\n",
    "# STEP 2: Encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# STEP 3: Image file path\n",
    "image_path = \"/Users/fatihwolf/Downloads/output (2)/row_6_teacher.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# STEP 4: Send request to GPT-4 Vision\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# STEP 5: Print the output\n",
    "print(\"Generated Caption:\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a8f49-7972-4203-9972-3bcfd812caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "\n",
      "main_objects: boy, soccer ball; main_object_attributes: young, blonde hair, blue jersey, black shorts; location: soccer field; action: dribbling; surroundings: goalposts, other soccer balls; background: blurred trees, house, blue sky\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# STEP 1: Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"\")  # Replace with your key\n",
    "\n",
    "# STEP 2: Encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# STEP 3: Image file path\n",
    "image_path = \"/Users/fatihwolf/Downloads/output (2)/row_6_student.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# STEP 4: Send request to GPT-4 Vision\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# STEP 5: Print the output\n",
    "print(\"Generated Caption:\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f135e-38b1-4fb4-b9ba-c600445b272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession, ClientTimeout, TCPConnector\n",
    "\n",
    "# Set your OpenAI API key\n",
    "API_KEY = \"API KEY HERE\"\n",
    "client = openai.AsyncOpenAI(api_key=API_KEY)\n",
    "\n",
    "FOLDER_PATH = \"FOLDER NAME HERE\"\n",
    "MAX_CONCURRENT_REQUESTS = 10  # Limit concurrent requests to prevent errors\n",
    "\n",
    "prompt_text = \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"\n",
    "\n",
    "# Load images asynchronously\n",
    "async def encode_image_async(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(await image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Function to send a request to OpenAI API\n",
    "async def process_image(image_path, session, semaphore):\n",
    "    async with semaphore:\n",
    "        image_base64 = await encode_image_async(image_path)\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "            ]}],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Process all images in parallel with controlled concurrency\n",
    "async def process_all_images():\n",
    "    image_files = sorted(f for f in os.listdir(FOLDER_PATH) if f.endswith(('.png', '.jpg', '.jpeg')))\n",
    "    \n",
    "    # Match teacher & student images\n",
    "    image_pairs = {}\n",
    "    for filename in image_files:\n",
    "        match = re.match(r\"row_(\\d+)_(teacher|student)\\.(png|jpg|jpeg)\", filename)\n",
    "        if match:\n",
    "            row_number, role, _ = match.groups()\n",
    "            if row_number not in image_pairs:\n",
    "                image_pairs[row_number] = {}\n",
    "            image_pairs[row_number][role] = filename\n",
    "\n",
    "    results = []\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    async with ClientSession(timeout=ClientTimeout(total=60), connector=TCPConnector(limit=MAX_CONCURRENT_REQUESTS)) as session:\n",
    "        tasks = []\n",
    "        \n",
    "        for row, images in sorted(image_pairs.items(), key=lambda x: int(x[0])):\n",
    "            teacher_img = images.get(\"teacher\")\n",
    "            student_img = images.get(\"student\")\n",
    "            if teacher_img and student_img:\n",
    "                teacher_path = os.path.join(FOLDER_PATH, teacher_img)\n",
    "                student_path = os.path.join(FOLDER_PATH, student_img)\n",
    "                \n",
    "                tasks.append(process_image(teacher_path, session, semaphore))\n",
    "                tasks.append(process_image(student_path, session, semaphore))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for i, (row, images) in enumerate(sorted(image_pairs.items(), key=lambda x: int(x[0]))):\n",
    "            results.append({\n",
    "                \"Row\": row,\n",
    "                \"Teacher Caption\": responses[i * 2],\n",
    "                \"Student Caption\": responses[i * 2 + 1]\n",
    "            })\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Processing complete! Results saved to 'results.csv'.\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(process_all_images())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR BIGGER DATA -- THERE IS A LIMIT PROBLEM\n",
    "\n",
    "import openai\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession, ClientTimeout, TCPConnector\n",
    "import random\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Set your OpenAI API key\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "client = openai.AsyncOpenAI(api_key=API_KEY)\n",
    "\n",
    "FOLDER_PATH = \"YOUR_IMAGES_FOLDER\"\n",
    "MAX_CONCURRENT_REQUESTS = 10  # Limit concurrent requests to prevent errors\n",
    "\n",
    "prompt_text = \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed. Do not include labels like \"cartoon cat\"—only the object itself.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"\n",
    "\n",
    "# Load images asynchronously\n",
    "async def encode_image_async(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(await image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "async def process_image(image_path, session, semaphore, max_retries=5):\n",
    "    async with semaphore:\n",
    "        image_base64 = await encode_image_async(image_path)\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                    ]}],\n",
    "                    max_tokens=300,\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "\n",
    "            except RateLimitError:\n",
    "                print(\"⏳ Rate limit hit. Waiting 60 seconds before retrying...\")\n",
    "                await asyncio.sleep(60)  # Hard wait instead of exponential backoff\n",
    "\n",
    "            except APIError as e:\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f\"⚠️ API error occurred: {e}. Retrying in {wait_time:.2f} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "\n",
    "        print(f\"❌ Failed to process image: {image_path} after {max_retries} attempts.\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# Process all images in parallel with controlled concurrency\n",
    "async def process_all_images():\n",
    "    image_files = sorted(f for f in os.listdir(FOLDER_PATH) if f.endswith(('.png', '.jpg', '.jpeg')))\n",
    "\n",
    "    # Match teacher & student images\n",
    "    image_pairs = {}\n",
    "    for filename in image_files:\n",
    "        match = re.match(r\"row_(\\d+)_(teacher|student)\\.(png|jpg|jpeg)\", filename)\n",
    "        if match:\n",
    "            row_number, role, _ = match.groups()\n",
    "            if row_number not in image_pairs:\n",
    "                image_pairs[row_number] = {}\n",
    "            image_pairs[row_number][role] = filename\n",
    "\n",
    "    results = []\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    async with ClientSession(timeout=ClientTimeout(total=60), connector=TCPConnector(limit=MAX_CONCURRENT_REQUESTS)) as session:\n",
    "        tasks = []\n",
    "        row_list = sorted(image_pairs.items(), key=lambda x: int(x[0]))\n",
    "\n",
    "        for row, images in row_list:\n",
    "            teacher_img = images.get(\"teacher\")\n",
    "            student_img = images.get(\"student\")\n",
    "            if teacher_img and student_img:\n",
    "                teacher_path = os.path.join(FOLDER_PATH, teacher_img)\n",
    "                student_path = os.path.join(FOLDER_PATH, student_img)\n",
    "\n",
    "                tasks.append(process_image(teacher_path, session, semaphore))\n",
    "                tasks.append(process_image(student_path, session, semaphore))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        for i, (row, _) in enumerate(row_list):\n",
    "            results.append({\n",
    "                \"Row\": row,\n",
    "                \"Teacher Caption\": responses[i * 2],\n",
    "                \"Student Caption\": responses[i * 2 + 1]\n",
    "            })\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"captions.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Processing complete! Results saved to 'captions.csv'.\")\n",
    "\n",
    "# Allow running in Jupyter/Colab without asyncio.run error\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await process_all_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession, ClientTimeout, TCPConnector\n",
    "import random\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Set your OpenAI API key\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "client = openai.AsyncOpenAI(api_key=API_KEY)\n",
    "\n",
    "FOLDER_PATH = \"IMAGES_FOLDER\"\n",
    "MAX_CONCURRENT_REQUESTS = 10  # Limit concurrent requests to prevent errors\n",
    "\n",
    "# Set the row range you want to process\n",
    "MIN_ROW = 0  # Inclusive start row\n",
    "MAX_ROW = 20  # Inclusive end row\n",
    "\n",
    "prompt_text = \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed. Do not include labels like \"cartoon cat\"—only the object itself.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"\n",
    "\n",
    "# Load images asynchronously\n",
    "async def encode_image_async(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(await image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "async def process_image(image_path, session, semaphore, max_retries=5):\n",
    "    async with semaphore:\n",
    "        image_base64 = await encode_image_async(image_path)\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                    ]}],\n",
    "                    max_tokens=300,\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "\n",
    "            except RateLimitError as e:\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f\"⚠️ Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "\n",
    "            except APIError as e:\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f\"⚠️ API error occurred: {e}. Retrying in {wait_time:.2f} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "\n",
    "        print(f\"❌ Failed to process image: {image_path} after {max_retries} attempts.\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# Process all images in parallel with controlled concurrency\n",
    "async def process_all_images():\n",
    "    image_files = sorted(f for f in os.listdir(FOLDER_PATH) if f.endswith(('.png', '.jpg', '.jpeg')))\n",
    "\n",
    "    # Match teacher & student images\n",
    "    image_pairs = {}\n",
    "    for filename in image_files:\n",
    "        match = re.match(r\"row_(\\d+)_(teacher|student)\\.(png|jpg|jpeg)\", filename)\n",
    "        if match:\n",
    "            row_number, role, _ = match.groups()\n",
    "            if row_number not in image_pairs:\n",
    "                image_pairs[row_number] = {}\n",
    "            image_pairs[row_number][role] = filename\n",
    "\n",
    "    results = []\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    async with ClientSession(timeout=ClientTimeout(total=60), connector=TCPConnector(limit=MAX_CONCURRENT_REQUESTS)) as session:\n",
    "        tasks = []\n",
    "\n",
    "        for row, images in sorted(image_pairs.items(), key=lambda x: int(x[0])):\n",
    "            row_num = int(row)\n",
    "            if not (MIN_ROW <= row_num <= MAX_ROW):\n",
    "                continue\n",
    "\n",
    "            teacher_img = images.get(\"teacher\")\n",
    "            student_img = images.get(\"student\")\n",
    "            if teacher_img and student_img:\n",
    "                teacher_path = os.path.join(FOLDER_PATH, teacher_img)\n",
    "                student_path = os.path.join(FOLDER_PATH, student_img)\n",
    "\n",
    "                tasks.append(process_image(teacher_path, session, semaphore))\n",
    "                tasks.append(process_image(student_path, session, semaphore))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        filtered_pairs = [(row, images) for row, images in sorted(image_pairs.items(), key=lambda x: int(x[0])) if MIN_ROW <= int(row) <= MAX_ROW]\n",
    "        for i, (row, images) in enumerate(filtered_pairs):\n",
    "            results.append({\n",
    "                \"Row\": row,\n",
    "                \"Teacher Caption\": responses[i * 2],\n",
    "                \"Student Caption\": responses[i * 2 + 1]\n",
    "            })\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"captions.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Processing complete! Results saved to 'captions.csv'.\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(process_all_images())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
