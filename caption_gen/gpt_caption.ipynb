{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5ec7dc-503d-43e1-87c7-f2281a75bbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/fatihwolf/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Downloading openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (319 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.9.0 openai-1.70.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f305c-e8d5-4d7b-bbd5-6f06b1dde17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "\n",
      "main_objects: boy, soccer ball; main_object_attributes: red jersey, blue shorts, black shoes, medium size ball; location: soccer field; action: kicking; surroundings: grass, goalposts; background: bright sun, blue sky with clouds, trees\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# STEP 1: Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"nn\")  # Replace with your key\n",
    "\n",
    "# STEP 2: Encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# STEP 3: Image file path\n",
    "image_path = \"/Users/fatihwolf/Downloads/output (2)/row_6_teacher.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# STEP 4: Send request to GPT-4 Vision\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# STEP 5: Print the output\n",
    "print(\"Generated Caption:\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a8f49-7972-4203-9972-3bcfd812caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      "\n",
      "main_objects: boy, soccer ball; main_object_attributes: young, blonde hair, blue jersey, black shorts; location: soccer field; action: dribbling; surroundings: goalposts, other soccer balls; background: blurred trees, house, blue sky\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# STEP 1: Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"\")  # Replace with your key\n",
    "\n",
    "# STEP 2: Encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# STEP 3: Image file path\n",
    "image_path = \"/Users/fatihwolf/Downloads/output (2)/row_6_student.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# STEP 4: Send request to GPT-4 Vision\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"\"\"You are a vision-language assistant. Your task is to describe an image using exactly six predefined categories. You MUST return your response in a **single line**, using the following key-value format:\n",
    "\n",
    "main_objects: ..., main_object_attributes: ..., location: ..., action: ..., surroundings: ..., background: ...\n",
    "\n",
    "Each category must be filled with detailed and specific visual content extracted from the image. Do NOT use vague terms like \"stuff,\" \"something,\" or \"object.\" Do not include commentary, explanations, greetings, or extra text. Follow this structure **exactly** and use commas to separate phrases **within** each category (not between categories). Here's what each category must include:\n",
    "\n",
    "- **main_objects**: The visually dominant object(s) in the image. Use nouns only (e.g., dog, man, car). Plural if needed.\n",
    "- **main_object_attributes**: Descriptive attributes of the main objects. Include color, size, shape, texture, pose, clothing, breed, etc. Be specific and avoid general adjectives like \"nice\" or \"good.\"\n",
    "- **location**: The specific place or physical setting of the main objects. It can be indoor (e.g., kitchen, classroom) or outdoor (e.g., beach, forest). Be precise.\n",
    "- **action**: The primary visible activity or behavior of the main object(s). Use verbs (e.g., eating, running, playing). If nothing is happening, say \"none.\"\n",
    "- **surroundings**: Notable objects, furniture, or elements near the main object(s). Focus on things in the foreground or immediate context.\n",
    "- **background**: What is behind the main objects — it can include landscape elements (e.g., mountains, sky), walls, scenery, or distant elements.\n",
    "\n",
    "Output Format Example (follow this EXACTLY, only change content):\n",
    "main_objects: dog, girl; main_object_attributes: golden retriever, medium size, fluffy fur, barefoot child in red dress; location: grassy park; action: playing fetch; surroundings: ball, picnic basket, tree; background: blue sky with clouds, distant buildings\n",
    "\"\"\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# STEP 5: Print the output\n",
    "print(\"Generated Caption:\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f135e-38b1-4fb4-b9ba-c600445b272f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
