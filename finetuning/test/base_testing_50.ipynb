{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f012b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1c3d00ad6c4b71abaed9c38466a300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Row 1 done.\n",
      "âœ… Row 2 done.\n",
      "âœ… Row 3 done.\n",
      "âœ… Row 4 done.\n",
      "âœ… Row 5 done.\n",
      "âœ… Row 6 done.\n",
      "âœ… Row 7 done.\n",
      "âœ… Row 8 done.\n",
      "âœ… Row 9 done.\n",
      "âœ… Row 10 done.\n",
      "âœ… Row 11 done.\n",
      "âœ… Row 12 done.\n",
      "âœ… Row 13 done.\n",
      "âœ… Row 14 done.\n",
      "âœ… Row 15 done.\n",
      "âœ… Row 16 done.\n",
      "âœ… Row 17 done.\n",
      "âœ… Row 18 done.\n",
      "âœ… Row 19 done.\n",
      "âœ… Row 20 done.\n",
      "âœ… Row 21 done.\n",
      "âœ… Row 22 done.\n",
      "âœ… Row 23 done.\n",
      "âœ… Row 24 done.\n",
      "âœ… Row 25 done.\n",
      "âœ… Row 26 done.\n",
      "âœ… Row 27 done.\n",
      "âœ… Row 28 done.\n",
      "âœ… Row 29 done.\n",
      "âœ… Row 30 done.\n",
      "âœ… Row 31 done.\n",
      "âœ… Row 32 done.\n",
      "âœ… Row 33 done.\n",
      "âœ… Row 34 done.\n",
      "âœ… Row 35 done.\n",
      "âœ… Row 36 done.\n",
      "âœ… Row 37 done.\n",
      "âœ… Row 38 done.\n",
      "âœ… Row 39 done.\n",
      "âœ… Row 40 done.\n",
      "âœ… Row 41 done.\n",
      "âœ… Row 42 done.\n",
      "âœ… Row 43 done.\n",
      "âœ… Row 44 done.\n",
      "âœ… Row 45 done.\n",
      "âœ… Row 46 done.\n",
      "âœ… Row 47 done.\n",
      "âœ… Row 48 done.\n",
      "âœ… Row 49 done.\n",
      "âœ… Row 50 done.\n",
      "\n",
      "ðŸ“„ Saved results to /workspace/50set/caption_results_base.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "\n",
    "# ---- CONFIG ----\n",
    "base_model_id = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
    "folder = Path(\"/workspace/50set\")\n",
    "output_csv = folder / \"caption_results_base.csv\"\n",
    "\n",
    "# ---- LOAD BASE MODEL ONLY ----\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# ---- LOAD PROCESSOR ----\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)\n",
    "\n",
    "# ---- INSTRUCTION ----\n",
    "instruction = (\n",
    "    \"You are a vision-language assistant. Describe an image using exactly six categories in a single line:\\n\\n\"\n",
    "    \"main_objects: ... main_object_attributes: ... location: ... action: ... surroundings: ... background: ...\\n\\n\"\n",
    "    \"Format rules:\\n\"\n",
    "    \"- Each category must start with its name, followed by a colon and a space\\n\"\n",
    "    \"- Use detailed, specific descriptions\\n\"\n",
    "    \"- Separate categories with a comma and a space\\n\"\n",
    "    \"- If a category is unclear, write: [category name]: none\\n\"\n",
    "    \"- Do NOT include commentary, line breaks, or extra text\\n\\n\"\n",
    "    \"Example:\\n\"\n",
    "    \"main_objects: rabbit main_object_attributes: brown fur, fluffy tail, big ears, happy expression location: meadow action: none surroundings: butterflies, flowers, green grass background: foliage, sunlight\"\n",
    ")\n",
    "\n",
    "# ---- HELPER FUNCTION ----\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}]}]\n",
    "    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ---- PROCESS BATCH ----\n",
    "results = []\n",
    "for i in range(1, 51):\n",
    "    index = str(i).zfill(3)\n",
    "    student_img = folder / f\"{index}_student.png\"\n",
    "    teacher_img = folder / f\"{index}_teacher.png\"\n",
    "\n",
    "    if student_img.exists() and teacher_img.exists():\n",
    "        student_caption = generate_caption(student_img)\n",
    "        teacher_caption = generate_caption(teacher_img)\n",
    "        results.append({\"Row\": i, \"Student\": student_caption, \"Teacher\": teacher_caption})\n",
    "        print(f\"âœ… Row {i} done.\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Missing pair for row {i}.\")\n",
    "\n",
    "# ---- SAVE TO CSV ----\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nðŸ“„ Saved results to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da9568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
